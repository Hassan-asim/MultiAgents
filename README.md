# AI Skill Bridge (AISB) - Intelligent Quiz & Video Assessment Platform

![Next.js](https://img.shields.io/badge/Next.js-15.5.4-black)
![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue)
![Supabase](https://img.shields.io/badge/Supabase-2.58.0-green)
![CrewAI](https://img.shields.io/badge/CrewAI-Powered-orange)
![Tailwind CSS](https://img.shields.io/badge/Tailwind_CSS-3.0-38B2AC)

## üöÄ Overview

AI Skill Bridge (AISB) is an advanced educational platform that combines intelligent quiz generation with AI-powered video assessment. The platform leverages CrewAI agents to automate the entire assessment workflow, from generating personalized quizzes to evaluating video submissions and ranking participants.

## üèóÔ∏è Architecture

The platform is built using modern web technologies and AI frameworks:

- **Frontend**: Next.js 15.5.4 with TypeScript and Tailwind CSS
- **Database**: Supabase with PostgreSQL
- **AI Framework**: CrewAI for intelligent agent orchestration
- **LLM Integration**: Groq for fast AI inference
- **Email Service**: Resend for automated notifications
- **UI Components**: Radix UI primitives with custom styling

## ü§ñ AI Agents & Tools

### Core Agents

The platform utilizes CrewAI-powered agents that work together to create a seamless assessment experience:

#### 1. Quiz Generator Agent
- **Purpose**: Creates personalized quizzes based on topics, difficulty, and requirements
- **Capabilities**:
  - Dynamic question generation from uploaded Excel data
  - Multiple difficulty levels (Easy, Medium, Hard)
  - Customizable topic coverage
  - Time-based question allocation
- **Tool**: `quizGeneratorTool`

#### 2. Video Processor Agent
- **Purpose**: Analyzes video submissions using transcript-based evaluation
- **Capabilities**:
  - YouTube video transcript extraction
  - Content relevance scoring (0-40 points)
  - Content quality assessment (0-30 points)
  - Presentation evaluation (0-20 points)
  - Engagement scoring (0-10 points)
- **Tool**: `videoProcessorTool`

#### 3. Score & Notify Agent
- **Purpose**: Calculates quiz scores and notifies top performers
- **Capabilities**:
  - Automated scoring of quiz submissions
  - Top 5 performer identification
  - Personalized email notifications
- **Tool**: `scoreAndNotifyTool`

#### 4. Final Ranking Agent
- **Purpose**: Combines quiz and video scores for final rankings
- **Capabilities**:
  - Comprehensive score calculation
  - Top 5% winner selection (minimum 1, maximum 10)
  - Prize tier assignment
  - Winner notification system
- **Tool**: `finalRankingTool`

#### 5. Email Communication Agent
- **Purpose**: Handles all automated email communications
- **Capabilities**:
  - Quiz invitations
  - Score notifications
  - Winner announcements
  - Custom template support
- **Tool**: `emailTool`

### Tool Ecosystem

Each agent utilizes specialized tools built with LangChain core:

```typescript
// Example Tool Structure
export const exampleTool = tool(
  async (input) => {
    // Tool logic implementation
    return 'Tool execution result';
  },
  {
    name: 'ExampleTool',
    description: 'Tool description for AI agent understanding',
    schema: z.object({
      // Input validation schema
    }),
  }
);
```

## üìä Platform Flow

### 1. Student Onboarding
```
Excel Upload ‚Üí Student Data Import ‚Üí Database Storage ‚Üí Email Invitations
```

### 2. Quiz Assessment Phase
```
Quiz Generation ‚Üí Student Participation ‚Üí Automated Scoring ‚Üí Top 5 Selection
```

### 3. Video Submission Phase
```
Video Upload ‚Üí Transcript Extraction ‚Üí AI Analysis ‚Üí Content Scoring
```

### 4. Final Evaluation
```
Score Combination ‚Üí Final Ranking ‚Üí Winner Selection ‚Üí Prize Distribution
```

### 5. Notification System
```
Automated Emails ‚Üí Score Reports ‚Üí Winner Announcements
```

## üéØ Key Features

### Admin Dashboard
- **Student Management**: Import, view, and manage student data
- **Quiz Creation**: Generate quizzes with AI assistance
- **Video Review**: Monitor and process video submissions
- **Results Analytics**: Comprehensive scoring and ranking views
- **Winner Management**: Track and notify competition winners
- **Settings Configuration**: Customize platform parameters

### Student Interface
- **Quiz Participation**: Time-based quiz taking with instant feedback
- **Video Submission**: YouTube link submission with metadata
- **Progress Tracking**: Real-time status updates

### Intelligent Assessment
- **Multi-Modal Evaluation**: Combines written and video assessments
- **AI-Powered Scoring**: Consistent and fair evaluation criteria
- **Automated Workflows**: Minimal manual intervention required

## üõ†Ô∏è Installation & Setup

### Prerequisites
- Node.js 18+ 
- npm or yarn
- Supabase account
- Groq API key
- Resend API key

### Environment Variables
Create a `.env.local` file with the following variables:

```env
# Supabase Configuration
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key

# AI Configuration
GROQ_API_KEY=your_groq_api_key

# Email Configuration
RESEND_API_KEY=your_resend_api_key
FROM_EMAIL=your_sender_email

# App Configuration
NEXTAUTH_SECRET=your_nextauth_secret
NEXTAUTH_URL=http://localhost:3000
```

### Installation Steps

1. **Clone the repository**
```bash
git clone <repository-url>
cd frontend
```

2. **Install dependencies**
```bash
npm install
```

3. **Set up database**
```bash
# Run the schema.sql file in your Supabase dashboard
# or use the Supabase CLI
supabase db reset
```

4. **Start development server**
```bash
npm run dev
```

5. **Build for production**
```bash
npm run build
npm start
```

## üóÑÔ∏è Database Schema

### Core Tables

#### Students
```sql
CREATE TABLE students (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

#### Quizzes
```sql
CREATE TABLE quizzes (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    num_questions INT NOT NULL,
    difficulty VARCHAR(50),
    topics TEXT[],
    time_per_question INT,
    type VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

#### Video Submissions
```sql
CREATE TABLE video_submissions (
    id SERIAL PRIMARY KEY,
    student_id INT REFERENCES students(id),
    youtube_link VARCHAR(255) NOT NULL,
    transcript TEXT,
    evaluation JSONB,
    score INT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

## üîß API Endpoints

### Quiz Management
- `POST /api/quizzes/generate` - Generate new quiz with AI
- `POST /api/quizzes/activate` - Activate/deactivate quizzes
- `POST /api/quizzes/score-and-notify` - Score quizzes and notify winners
- `POST /api/quizzes/send-invitations` - Send quiz invitations

### Video Processing
- `POST /api/videos/process` - Process video submissions
- `PUT /api/videos/process` - Process single video
- `POST /api/videos/final-rank-and-notify` - Final ranking and notifications

### Student Management
- `POST /api/students/upload` - Bulk student upload from Excel
- `GET /api/students` - Retrieve student data

### Authentication
- `POST /api/admin/login` - Admin authentication

## üé® UI Components

Built with Radix UI and styled with Tailwind CSS:

- **Forms**: React Hook Form with Zod validation
- **Tables**: Sortable data tables with pagination
- **Modals**: Accessible dialog components
- **Navigation**: Responsive sidebar navigation
- **Notifications**: Toast notifications with react-hot-toast
- **Loading States**: Skeleton loaders and progress indicators

## üìà Assessment Criteria

### Quiz Scoring
- **Automatic Calculation**: Based on correct answers
- **Time Consideration**: Bonus points for faster completion
- **Top 5 Selection**: Highest scoring participants advance

### Video Evaluation Metrics
1. **Relevance Score (40%)**: Content alignment with quiz topics
2. **Content Quality (30%)**: Educational value and depth
3. **Presentation (20%)**: Communication clarity and style
4. **Engagement (10%)**: Content appeal and informativeness

### Final Ranking Algorithm
```
Final Score = Quiz Score (60%) + Video Score (40%)
Winners = Top 5% of participants (min: 1, max: 10)
```

## üèÜ Competition Management

### Prize Structure
- **1st Place**: Winner Certificate + Recognition
- **2nd Place**: Runner-up Certificate
- **3rd Place**: Third Place Certificate
- **Participation**: Certificate of Participation

### Automated Workflows
1. **Student Import**: Excel file processing and email generation
2. **Quiz Distribution**: Automated invitation sending
3. **Score Processing**: Real-time calculation and ranking
4. **Winner Selection**: Algorithm-based top performer identification
5. **Notification System**: Automated congratulatory emails

## üîí Security Features

- **Data Validation**: Zod schema validation on all inputs
- **SQL Injection Protection**: Parameterized queries via Supabase
- **Type Safety**: Full TypeScript implementation
- **Environment Security**: Secure environment variable handling
- **Rate Limiting**: API endpoint protection

## üöÄ Deployment

### Vercel Deployment
1. Connect your GitHub repository to Vercel
2. Configure environment variables in Vercel dashboard
3. Deploy with automatic builds on push

### Environment Configuration
Ensure all environment variables are properly set in your deployment platform.

## üîÑ Development Workflow

### Code Quality
- **ESLint**: Comprehensive linting rules
- **TypeScript**: Strict type checking
- **Prettier**: Consistent code formatting
- **Husky**: Pre-commit hooks for quality assurance

### Testing Strategy
- **Component Testing**: React Testing Library
- **API Testing**: Jest for endpoint validation
- **E2E Testing**: Playwright for user journey testing

## üìù Contributing

1. Fork the repository
2. Create a feature branch
3. Implement changes with proper TypeScript types
4. Add comprehensive tests
5. Submit a pull request with description

## üìû Support

For technical support or questions:
- Create an issue in the repository
- Check the documentation
- Review the API endpoints

## üîÆ Future Enhancements

- **Real-time Collaboration**: Live quiz participation
- **Advanced Analytics**: Detailed performance insights
- **Multi-language Support**: Internationalization
- **Mobile App**: React Native implementation
- **Advanced AI Models**: Enhanced evaluation capabilities

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**Built with ‚ù§Ô∏è using CrewAI, Next.js, and modern web technologies**
# aisb-auto
# AI Skill Bridge (AISB) - Advanced Multi-Agent Educational Assessment Platform

![Next.js](https://img.shields.io/badge/Next.js-15.5.4-black)
![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue)
![Supabase](https://img.shields.io/badge/Supabase-2.58.0-green)
![CrewAI](https://img.shields.io/badge/CrewAI-Powered-orange)
![LangChain](https://img.shields.io/badge/LangChain-0.3.34-purple)
![Tailwind CSS](https://img.shields.io/badge/Tailwind_CSS-4.0-38B2AC)
![Groq](https://img.shields.io/badge/Groq-LLM-00A67E)

## üöÄ Executive Summary

AI Skill Bridge (AISB) is a revolutionary educational platform that leverages **CrewAI's advanced multi-agent orchestration framework** to create an intelligent, automated assessment ecosystem. The platform combines cutting-edge AI agents with modern web technologies to deliver personalized quiz generation, intelligent video evaluation, automated scoring, and comprehensive student ranking systems.

## üèóÔ∏è CrewAI-Powered Architecture

### Core CrewAI Framework Integration

The platform is built on **CrewAI's sophisticated multi-agent orchestration system**, which enables seamless coordination between specialized AI agents. Each agent operates as an autonomous entity with specific roles, tools, and communication protocols, working together to create a comprehensive educational assessment pipeline.

#### CrewAI Agent Hierarchy

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CREWAI ORCHESTRATOR                     ‚îÇ
‚îÇ                  (Central Coordination Hub)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ             ‚îÇ             ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Quiz    ‚îÇ   ‚îÇ Video   ‚îÇ   ‚îÇ Scoring ‚îÇ
   ‚îÇGenerator ‚îÇ   ‚îÇProcessor‚îÇ   ‚îÇ  Agent  ‚îÇ
   ‚îÇ  Agent   ‚îÇ   ‚îÇ  Agent  ‚îÇ   ‚îÇ         ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ             ‚îÇ             ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Ranking ‚îÇ   ‚îÇ Email   ‚îÇ   ‚îÇ Admin   ‚îÇ
   ‚îÇ  Agent  ‚îÇ   ‚îÇ  Agent  ‚îÇ   ‚îÇ  Agent  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### CrewAI Agent Specifications

#### 1. Quiz Generator Agent (CrewAI Orchestrated)
- **Agent Role**: Educational Content Specialist
- **Primary Function**: Dynamic quiz generation with adaptive difficulty
- **CrewAI Configuration**:
  ```python
  quiz_generator_agent = Agent(
      role='Educational Content Specialist',
      goal='Generate personalized, high-quality quizzes based on educational requirements',
      backstory='Expert educator with 20+ years of experience in curriculum design',
      verbose=True,
      allow_delegation=False,
      tools=[quiz_generator_tool],
      llm=llm_instance
  )
  ```
- **Capabilities**:
  - **Adaptive Question Generation**: Creates questions based on topic complexity and student level
  - **Multi-Format Support**: Multiple choice, true/false, and open-ended questions
  - **Difficulty Scaling**: Automatic difficulty adjustment based on topic complexity
  - **Time Optimization**: Calculates optimal time allocation per question
  - **Content Validation**: Ensures question quality and educational value

#### 2. Video Processor Agent (CrewAI Orchestrated)
- **Agent Role**: Multimedia Assessment Specialist
- **Primary Function**: Intelligent video content analysis and evaluation
- **CrewAI Configuration**:
  ```python
  video_processor_agent = Agent(
      role='Multimedia Assessment Specialist',
      goal='Analyze video submissions for educational content quality and relevance',
      backstory='Expert in educational technology and multimedia assessment',
      verbose=True,
      allow_delegation=True,
      tools=[video_processor_tool, transcript_extraction_tool],
      llm=llm_instance
  )
  ```
- **Advanced Capabilities**:
  - **Transcript Extraction**: Automated YouTube video transcript retrieval
  - **Content Relevance Analysis**: 40-point scoring system for topic alignment
  - **Educational Value Assessment**: 30-point evaluation of content depth
  - **Presentation Quality**: 20-point communication effectiveness scoring
  - **Engagement Metrics**: 10-point audience engagement analysis
  - **Real-time Processing**: Batch and individual video processing capabilities

#### 3. Score & Notify Agent (CrewAI Orchestrated)
- **Agent Role**: Assessment Coordinator
- **Primary Function**: Automated scoring and performance notification
- **CrewAI Configuration**:
  ```python
  score_notify_agent = Agent(
      role='Assessment Coordinator',
      goal='Calculate accurate scores and notify top performers',
      backstory='Experienced assessment coordinator with expertise in educational analytics',
      verbose=True,
      allow_delegation=True,
      tools=[score_calculator_tool, email_notification_tool],
      llm=llm_instance
  )
  ```
- **Intelligent Features**:
  - **Automated Scoring**: Real-time quiz answer evaluation
  - **Performance Analytics**: Statistical analysis of student performance
  - **Top Performer Identification**: Algorithm-based ranking system
  - **Personalized Notifications**: Customized email communications
  - **Progress Tracking**: Continuous monitoring of student improvement

#### 4. Final Ranking Agent (CrewAI Orchestrated)
- **Agent Role**: Competition Manager
- **Primary Function**: Comprehensive ranking and winner selection
- **CrewAI Configuration**:
  ```python
  final_ranking_agent = Agent(
      role='Competition Manager',
      goal='Determine final rankings and select competition winners',
      backstory='Expert in educational competitions and fair assessment practices',
      verbose=True,
      allow_delegation=False,
      tools=[ranking_calculator_tool, winner_selection_tool],
      llm=llm_instance
  )
  ```
- **Advanced Ranking System**:
  - **Multi-Modal Scoring**: Combines quiz (60%) and video (40%) scores
  - **Statistical Analysis**: Advanced algorithms for fair ranking
  - **Winner Selection**: Top 5% selection with minimum/maximum constraints
  - **Prize Distribution**: Automated prize tier assignment
  - **Transparency Reporting**: Detailed ranking explanations

#### 5. Email Communication Agent (CrewAI Orchestrated)
- **Agent Role**: Communication Specialist
- **Primary Function**: Automated email management and notifications
- **CrewAI Configuration**:
  ```python
  email_agent = Agent(
      role='Communication Specialist',
      goal='Manage all automated email communications with students and administrators',
      backstory='Expert in educational communication and student engagement',
      verbose=True,
      allow_delegation=False,
      tools=[email_tool, template_manager_tool],
      llm=llm_instance
  )
  ```
- **Communication Features**:
  - **Template Management**: Dynamic email template generation
  - **Personalization**: Student-specific content customization
  - **Delivery Tracking**: Email delivery confirmation and analytics
  - **Multi-Language Support**: Internationalization capabilities
  - **Bulk Operations**: Efficient mass communication handling

### CrewAI Task Orchestration

The platform implements **CrewAI's sophisticated task orchestration system** to coordinate complex workflows:

```python
# CrewAI Task Definition Example
quiz_generation_task = Task(
    description="""
    Generate a comprehensive quiz based on the following parameters:
    - Number of questions: {num_questions}
    - Difficulty level: {difficulty}
    - Topics: {topics}
    - Time per question: {time_per_question} seconds
    - Question type: {type}
    
    Ensure questions are educationally valuable, properly formatted,
    and aligned with the specified difficulty level.
    """,
    agent=quiz_generator_agent,
    expected_output="JSON array of question objects with proper structure",
    tools=[quiz_generator_tool]
)

video_processing_task = Task(
    description="""
    Process video submission and provide comprehensive evaluation:
    - Extract transcript from YouTube video
    - Analyze content relevance to quiz topics
    - Evaluate educational value and presentation quality
    - Calculate engagement metrics
    - Provide detailed feedback and scoring
    """,
    agent=video_processor_agent,
    expected_output="Structured evaluation report with scores and feedback",
    tools=[video_processor_tool, transcript_extraction_tool]
)
```

### CrewAI Crew Configuration

The platform utilizes **CrewAI's crew management system** for coordinated agent execution:

```python
# Main CrewAI Crew Definition
assessment_crew = Crew(
    agents=[
        quiz_generator_agent,
        video_processor_agent,
        score_notify_agent,
        final_ranking_agent,
        email_agent
    ],
    tasks=[
        quiz_generation_task,
        video_processing_task,
        scoring_task,
        ranking_task,
        notification_task
    ],
    process=Process.sequential,  # Sequential execution for dependent tasks
    verbose=True,
    memory=True,  # Enable crew memory for context retention
    planning=True  # Enable automatic task planning
)
```

## üõ†Ô∏è CrewAI Installation & Setup

### Prerequisites for CrewAI Implementation

Before setting up the CrewAI-powered platform, ensure you have the following prerequisites:

- **Python 3.8+** (Required for CrewAI)
- **Node.js 18+** (For frontend development)
- **Supabase Account** (Database and authentication)
- **Groq API Key** (LLM provider)
- **Resend API Key** (Email service)
- **OpenAI API Key** (Alternative LLM provider)

### CrewAI Installation Process

#### Step 1: Python Environment Setup
```bash
# Create virtual environment
python -m venv aisb-crewai-env

# Activate environment (Windows)
aisb-crewai-env\Scripts\activate

# Activate environment (macOS/Linux)
source aisb-crewai-env/bin/activate

# Upgrade pip
pip install --upgrade pip
```

#### Step 2: CrewAI Core Installation
```bash
# Install CrewAI framework
pip install crewai

# Install additional CrewAI dependencies
pip install crewai[tools]
pip install crewai[langchain]
pip install crewai[openai]

# Install LangChain components
pip install langchain
pip install langchain-openai
pip install langchain-groq
pip install langchain-core

# Install additional dependencies
pip install pydantic
pip install python-dotenv
pip install requests
pip install beautifulsoup4
pip install youtube-transcript-api
```

#### Step 3: Frontend Dependencies
```bash
# Navigate to frontend directory
cd frontend

# Install Node.js dependencies
npm install

# Install additional packages for CrewAI integration
npm install @crewai/core
npm install @crewai/tools
npm install axios
npm install form-data
```

#### Step 4: Environment Configuration
Create a comprehensive `.env` file with CrewAI-specific configurations:

```env
# CrewAI Configuration
CREWAI_VERBOSE=true
CREWAI_MEMORY_ENABLED=true
CREWAI_PLANNING_ENABLED=true
CREWAI_MAX_ITERATIONS=10
CREWAI_MAX_EXECUTION_TIME=300

# LLM Configuration
OPENAI_API_KEY=your_openai_api_key
GROQ_API_KEY=your_groq_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key

# CrewAI Agent Configuration
QUIZ_GENERATOR_MODEL=gpt-4
VIDEO_PROCESSOR_MODEL=gpt-4-vision-preview
SCORING_AGENT_MODEL=gpt-3.5-turbo
RANKING_AGENT_MODEL=gpt-4
EMAIL_AGENT_MODEL=gpt-3.5-turbo

# Database Configuration
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key

# Email Configuration
RESEND_API_KEY=your_resend_api_key
FROM_EMAIL=noreply@yourdomain.com

# Application Configuration
NEXTAUTH_SECRET=your_nextauth_secret
NEXTAUTH_URL=http://localhost:3000
ADMIN_PASSWORD=your_admin_password
```

### CrewAI Project Structure

The platform follows CrewAI's recommended project structure:

```
aisb-auto-main/
‚îú‚îÄ‚îÄ crewai/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quiz_generator_agent.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ video_processor_agent.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ score_notify_agent.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ final_ranking_agent.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ email_agent.py
‚îÇ   ‚îú‚îÄ‚îÄ tasks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quiz_generation_task.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ video_processing_task.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scoring_task.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ranking_task.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ notification_task.py
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quiz_generator_tool.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ video_processor_tool.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ score_calculator_tool.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ranking_tool.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ email_tool.py
‚îÇ   ‚îú‚îÄ‚îÄ crews/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assessment_crew.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main_crew.py
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ llm_config.py
‚îÇ       ‚îî‚îÄ‚îÄ agent_config.py
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îî‚îÄ‚îÄ requirements.txt
```

### CrewAI Agent Implementation Example

Here's a detailed example of how CrewAI agents are implemented in the platform:

```python
# crewai/agents/quiz_generator_agent.py
from crewai import Agent
from crewai.tools import BaseTool
from langchain_openai import ChatOpenAI
from .tools.quiz_generator_tool import QuizGeneratorTool

class QuizGeneratorAgent:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0.7,
            max_tokens=2000
        )
        
        self.agent = Agent(
            role='Educational Content Specialist',
            goal='Generate high-quality, personalized quizzes that align with educational objectives',
            backstory="""You are an expert educator with over 20 years of experience in curriculum design. 
            You specialize in creating engaging, challenging, and educationally valuable assessments 
            that accurately measure student understanding and knowledge retention.""",
            verbose=True,
            allow_delegation=False,
            tools=[QuizGeneratorTool()],
            llm=self.llm,
            max_iterations=5,
            max_execution_time=120
        )
    
    def generate_quiz(self, parameters):
        """Generate a quiz using CrewAI agent orchestration"""
        task_description = f"""
        Create a comprehensive quiz with the following specifications:
        - Number of questions: {parameters['num_questions']}
        - Difficulty level: {parameters['difficulty']}
        - Topics: {', '.join(parameters['topics'])}
        - Time per question: {parameters['time_per_question']} seconds
        - Question type: {parameters['type']}
        
        Ensure each question:
        1. Is educationally valuable and tests genuine understanding
        2. Is properly formatted with clear, unambiguous options
        3. Has a single correct answer
        4. Is appropriate for the specified difficulty level
        5. Relates directly to the specified topics
        
        Return the questions in the exact JSON format required by the system.
        """
        
        result = self.agent.execute(task_description)
        return result
```

### CrewAI Tool Implementation

CrewAI tools are implemented with comprehensive error handling and validation:

```python
# crewai/tools/quiz_generator_tool.py
from crewai.tools import BaseTool
from typing import Type, Dict, Any
from pydantic import BaseModel, Field
import json
import requests

class QuizGeneratorToolInput(BaseModel):
    num_questions: int = Field(description="Number of questions to generate")
    difficulty: str = Field(description="Difficulty level (Easy, Medium, Hard)")
    topics: list = Field(description="List of topics to cover")
    time_per_question: int = Field(description="Time allocated per question in seconds")
    question_type: str = Field(description="Type of questions (multiple-choice, true-false)")

class QuizGeneratorTool(BaseTool):
    name: str = "quiz_generator_tool"
    description: str = "Generates educational quizzes based on specified parameters"
    args_schema: Type[BaseModel] = QuizGeneratorToolInput
    
    def _run(self, **kwargs) -> str:
        """Execute the quiz generation tool"""
        try:
            # Validate input parameters
            input_data = QuizGeneratorToolInput(**kwargs)
            
            # Prepare prompt for LLM
            prompt = self._create_generation_prompt(input_data)
            
            # Call LLM API
            response = self._call_llm_api(prompt)
            
            # Validate and format response
            formatted_questions = self._validate_and_format_questions(response)
            
            return json.dumps(formatted_questions)
            
        except Exception as e:
            return f"Error generating quiz: {str(e)}"
    
    def _create_generation_prompt(self, input_data: QuizGeneratorToolInput) -> str:
        """Create detailed prompt for quiz generation"""
        return f"""
        Generate {input_data.num_questions} {input_data.question_type} questions about {', '.join(input_data.topics)}.
        
        Difficulty: {input_data.difficulty}
        Time per question: {input_data.time_per_question} seconds
        
        Requirements:
        1. Each question must test genuine understanding, not just memorization
        2. Options must be plausible and well-distributed
        3. Questions should progress from basic to advanced concepts
        4. Include questions that test application and analysis skills
        
        Return ONLY a valid JSON array with this exact structure:
        [
            {{
                "question_text": "The question text here",
                "options": ["Option A", "Option B", "Option C", "Option D"],
                "correct_answer": "0"
            }}
        ]
        """
    
    def _call_llm_api(self, prompt: str) -> str:
        """Call the LLM API with the generated prompt"""
        # Implementation would call OpenAI/Groq API
        pass
    
    def _validate_and_format_questions(self, response: str) -> list:
        """Validate and format the LLM response"""
        try:
            questions = json.loads(response)
            if not isinstance(questions, list):
                raise ValueError("Response must be a list of questions")
            
            for question in questions:
                self._validate_question_structure(question)
            
            return questions
        except json.JSONDecodeError:
            raise ValueError("Invalid JSON response from LLM")
    
    def _validate_question_structure(self, question: dict) -> None:
        """Validate individual question structure"""
        required_fields = ['question_text', 'options', 'correct_answer']
        for field in required_fields:
            if field not in question:
                raise ValueError(f"Missing required field: {field}")
        
        if not isinstance(question['options'], list) or len(question['options']) < 2:
            raise ValueError("Options must be a list with at least 2 items")
```

## üéØ Platform Features & Capabilities

### Advanced Admin Dashboard

The admin dashboard provides comprehensive control over the entire assessment ecosystem:

#### Student Management System
- **Bulk Import**: Excel-based student data import with validation
- **Data Validation**: Automatic email format and data integrity checking
- **Student Profiles**: Comprehensive student information management
- **Progress Tracking**: Real-time student performance monitoring
- **Communication History**: Complete email interaction logs

#### Quiz Management Interface
- **Dynamic Generation**: AI-powered quiz creation with customizable parameters
- **Template System**: Reusable quiz templates for consistent assessment
- **Difficulty Scaling**: Automatic difficulty adjustment based on topic complexity
- **Time Management**: Intelligent time allocation per question
- **Quality Assurance**: Built-in question validation and review system

#### Video Assessment Center
- **Batch Processing**: Simultaneous processing of multiple video submissions
- **Transcript Analysis**: Automated YouTube video transcript extraction
- **Content Evaluation**: Multi-dimensional scoring system (relevance, quality, presentation, engagement)
- **Real-time Feedback**: Instant evaluation results and detailed feedback
- **Progress Monitoring**: Live processing status and completion tracking

#### Results Analytics Dashboard
- **Performance Metrics**: Comprehensive scoring and ranking analytics
- **Statistical Analysis**: Detailed performance statistics and trends
- **Comparative Analysis**: Student performance comparison tools
- **Export Capabilities**: Data export in multiple formats (CSV, PDF, Excel)
- **Visualization**: Interactive charts and graphs for data interpretation

#### Winner Management System
- **Automated Selection**: Algorithm-based winner identification
- **Prize Distribution**: Automated prize tier assignment and tracking
- **Notification System**: Personalized winner announcement emails
- **Certificate Generation**: Automated certificate creation and distribution
- **Recognition Tracking**: Complete winner history and achievement records

### Student Interface Features

#### Interactive Quiz Platform
- **Responsive Design**: Mobile-optimized quiz interface
- **Real-time Timer**: Live countdown timer with automatic submission
- **Progress Tracking**: Visual progress indicators and completion status
- **Instant Feedback**: Immediate scoring and explanation display
- **Accessibility**: WCAG-compliant interface for inclusive access

#### Video Submission System
- **YouTube Integration**: Seamless YouTube link submission
- **Metadata Collection**: Automatic video information extraction
- **Submission Validation**: URL format and accessibility verification
- **Status Updates**: Real-time submission status and processing updates
- **Resubmission Support**: Ability to update submissions before deadline

#### Progress Dashboard
- **Performance Overview**: Comprehensive score and ranking display
- **Historical Data**: Complete assessment history and progress tracking
- **Achievement Badges**: Gamification elements for student engagement
- **Goal Setting**: Personal performance targets and achievement tracking
- **Social Features**: Leaderboards and peer comparison tools

### Intelligent Assessment Engine

#### Multi-Modal Evaluation System
- **Written Assessment**: Comprehensive quiz-based knowledge evaluation
- **Video Assessment**: Multimedia content analysis and evaluation
- **Combined Scoring**: Weighted scoring system (60% quiz, 40% video)
- **Adaptive Difficulty**: Dynamic difficulty adjustment based on performance
- **Fair Evaluation**: Bias-free assessment algorithms

#### AI-Powered Analysis
- **Content Relevance**: Advanced topic alignment analysis
- **Educational Value**: Depth and quality of educational content assessment
- **Presentation Quality**: Communication effectiveness evaluation
- **Engagement Metrics**: Audience engagement and informativeness analysis
- **Consistency Scoring**: Standardized evaluation criteria across all submissions

#### Automated Workflow Management
- **Sequential Processing**: Coordinated agent execution for complex workflows
- **Error Handling**: Comprehensive error detection and recovery mechanisms
- **Quality Assurance**: Multi-level validation and verification systems
- **Performance Optimization**: Efficient resource utilization and processing
- **Scalability**: Support for large-scale assessment operations

## üóÑÔ∏è Database Architecture

### Supabase Integration

The platform leverages Supabase's powerful PostgreSQL database with advanced features:

#### Core Tables Structure

```sql
-- Enhanced Students Table with CrewAI Integration
CREATE TABLE students (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    student_id VARCHAR(50) UNIQUE,
    grade_level VARCHAR(20),
    institution VARCHAR(255),
    enrollment_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    last_activity TIMESTAMP WITH TIME ZONE,
    assessment_count INTEGER DEFAULT 0,
    total_score DECIMAL(10,2) DEFAULT 0,
    average_score DECIMAL(5,2) DEFAULT 0,
    is_active BOOLEAN DEFAULT TRUE,
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Advanced Quizzes Table with CrewAI Configuration
CREATE TABLE quizzes (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    num_questions INT NOT NULL,
    difficulty VARCHAR(50) NOT NULL,
    topics TEXT[] NOT NULL,
    time_per_question INT NOT NULL,
    total_time INT GENERATED ALWAYS AS (num_questions * time_per_question) STORED,
    type VARCHAR(50) NOT NULL,
    is_active BOOLEAN DEFAULT FALSE,
    is_published BOOLEAN DEFAULT FALSE,
    start_date TIMESTAMP WITH TIME ZONE,
    end_date TIMESTAMP WITH TIME ZONE,
    max_attempts INTEGER DEFAULT 1,
    passing_score INTEGER DEFAULT 60,
    crewai_config JSONB,
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Comprehensive Questions Table
CREATE TABLE questions (
    id SERIAL PRIMARY KEY,
    quiz_id INT REFERENCES quizzes(id) ON DELETE CASCADE,
    question_text TEXT NOT NULL,
    question_type VARCHAR(50) NOT NULL,
    options JSONB NOT NULL,
    correct_answer VARCHAR(255) NOT NULL,
    explanation TEXT,
    difficulty_level VARCHAR(20),
    topic_tags TEXT[],
    points INTEGER DEFAULT 1,
    time_limit INTEGER,
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Detailed Answers Table with Analytics
CREATE TABLE answers (
    id SERIAL PRIMARY KEY,
    student_id INT REFERENCES students(id) ON DELETE CASCADE,
    question_id INT REFERENCES questions(id) ON DELETE CASCADE,
    quiz_id INT REFERENCES quizzes(id) ON DELETE CASCADE,
    answer_text TEXT NOT NULL,
    is_correct BOOLEAN NOT NULL,
    time_taken INTEGER,
    attempt_number INTEGER DEFAULT 1,
    confidence_level INTEGER,
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Advanced Video Submissions Table
CREATE TABLE video_submissions (
    id SERIAL PRIMARY KEY,
    student_id INT REFERENCES students(id) ON DELETE CASCADE,
    quiz_id INT REFERENCES quizzes(id) ON DELETE CASCADE,
    youtube_link VARCHAR(500) NOT NULL,
    video_id VARCHAR(50),
    video_title VARCHAR(255),
    video_duration INTEGER,
    transcript TEXT,
    analysis_data JSONB,
    relevance_score INTEGER,
    content_quality_score INTEGER,
    presentation_score INTEGER,
    engagement_score INTEGER,
    total_score INTEGER,
    status VARCHAR(50) DEFAULT 'pending',
    processing_started_at TIMESTAMP WITH TIME ZONE,
    processed_at TIMESTAMP WITH TIME ZONE,
    error_message TEXT,
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Assessment Results Table
CREATE TABLE assessment_results (
    id SERIAL PRIMARY KEY,
    student_id INT REFERENCES students(id) ON DELETE CASCADE,
    quiz_id INT REFERENCES quizzes(id) ON DELETE CASCADE,
    quiz_score INTEGER NOT NULL,
    video_score INTEGER,
    total_score DECIMAL(10,2) NOT NULL,
    percentage DECIMAL(5,2) NOT NULL,
    rank INTEGER,
    percentile DECIMAL(5,2),
    completion_time TIMESTAMP WITH TIME ZONE,
    is_winner BOOLEAN DEFAULT FALSE,
    prize_tier VARCHAR(50),
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Email Communications Log
CREATE TABLE email_logs (
    id SERIAL PRIMARY KEY,
    recipient_email VARCHAR(255) NOT NULL,
    recipient_name VARCHAR(255),
    email_type VARCHAR(50) NOT NULL,
    subject VARCHAR(255) NOT NULL,
    template_used VARCHAR(100),
    status VARCHAR(20) DEFAULT 'pending',
    sent_at TIMESTAMP WITH TIME ZONE,
    delivered_at TIMESTAMP WITH TIME ZONE,
    opened_at TIMESTAMP WITH TIME ZONE,
    clicked_at TIMESTAMP WITH TIME ZONE,
    error_message TEXT,
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

#### Advanced Database Features

- **Real-time Subscriptions**: Live updates for assessment progress
- **Row Level Security**: Comprehensive data protection and access control
- **Automated Backups**: Regular database backup and recovery systems
- **Performance Optimization**: Indexed queries and query optimization
- **Data Validation**: Comprehensive constraint and validation rules

## üîß API Endpoints Documentation

### CrewAI-Integrated API Architecture

The platform exposes comprehensive RESTful APIs that integrate seamlessly with CrewAI agents:

#### Quiz Management Endpoints

##### Generate Quiz
```http
POST /api/quizzes/generate
Content-Type: application/json

{
  "quizId": 1,
  "numQuestions": 10,
  "difficulty": "Medium",
  "topics": ["AI", "Machine Learning", "Data Science"],
  "timePerQuestion": 60,
  "type": "multiple-choice"
}
```

**Response:**
```json
{
  "success": true,
  "message": "Quiz generated successfully",
  "data": {
    "quizId": 1,
    "questionsGenerated": 10,
    "processingTime": "2.3s",
    "crewaiTaskId": "task_12345"
  }
}
```

##### Activate Quiz
```http
POST /api/quizzes/activate
Content-Type: application/json

{
  "quizId": 1,
  "startDate": "2024-01-15T09:00:00Z",
  "endDate": "2024-01-15T17:00:00Z"
}
```

##### Send Invitations
```http
POST /api/quizzes/send-invitations
Content-Type: application/json

{
  "quizId": 1,
  "studentIds": [1, 2, 3, 4, 5],
  "template": "quiz_invitation",
  "personalize": true
}
```

#### Video Processing Endpoints

##### Process Video Submissions
```http
POST /api/videos/process
Content-Type: application/json

{
  "submissionIds": [1, 2, 3]
}
```

**Response:**
```json
{
  "success": true,
  "processed": 3,
  "failed": 0,
  "results": [
    {
      "submissionId": 1,
      "studentId": 1,
      "totalScore": 85,
      "breakdown": {
        "relevanceScore": 35,
        "contentQualityScore": 25,
        "presentationScore": 15,
        "engagementScore": 10
      },
      "processingTime": "45.2s"
    }
  ]
}
```

##### Process Single Video
```http
PUT /api/videos/process
Content-Type: application/json

{
  "submissionId": 1
}
```

#### Scoring and Ranking Endpoints

##### Score and Notify
```http
POST /api/quizzes/score-and-notify
Content-Type: application/json

{
  "quizId": 1,
  "notifyTop": 5,
  "includeFeedback": true
}
```

##### Final Ranking
```http
POST /api/videos/final-rank-and-notify
Content-Type: application/json

{
  "quizId": 1,
  "winnerPercentage": 5,
  "minWinners": 1,
  "maxWinners": 10
}
```

#### Student Management Endpoints

##### Upload Students
```http
POST /api/students/upload
Content-Type: multipart/form-data

{
  "file": "students.xlsx",
  "validateEmails": true,
  "sendWelcomeEmails": true
}
```

#### Winner Management Endpoints

##### Notify Winners
```http
POST /api/winners/notify
Content-Type: application/json

{
  "quizId": 1,
  "includeCertificates": true,
  "template": "winner_announcement"
}
```

### API Response Standards

All API endpoints follow consistent response patterns:

#### Success Response
```json
{
  "success": true,
  "message": "Operation completed successfully",
  "data": {
    // Response data
  },
  "metadata": {
    "timestamp": "2024-01-15T10:30:00Z",
    "processingTime": "1.2s",
    "crewaiTaskId": "task_12345"
  }
}
```

#### Error Response
```json
{
  "success": false,
  "error": "Error message",
  "code": "ERROR_CODE",
  "details": {
    // Additional error details
  },
  "metadata": {
    "timestamp": "2024-01-15T10:30:00Z",
    "requestId": "req_12345"
  }
}
```

## üöÄ Deployment Guide

### Production Deployment with CrewAI

#### Environment Preparation

1. **Server Requirements**
   - **CPU**: Minimum 4 cores, recommended 8+ cores
   - **RAM**: Minimum 8GB, recommended 16GB+
   - **Storage**: Minimum 50GB SSD
   - **Network**: Stable internet connection for API calls

2. **Dependencies Installation**
```bash
# Update system packages
sudo apt update && sudo apt upgrade -y

# Install Python 3.8+
sudo apt install python3.8 python3.8-venv python3-pip

# Install Node.js 18+
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt-get install -y nodejs

# Install PostgreSQL (if not using Supabase)
sudo apt install postgresql postgresql-contrib

# Install Redis (for CrewAI caching)
sudo apt install redis-server
```

#### CrewAI Production Configuration

1. **CrewAI Environment Setup**
```bash
# Create production environment
python3 -m venv /opt/aisb/crewai-env
source /opt/aisb/crewai-env/bin/activate

# Install production dependencies
pip install --upgrade pip
pip install crewai[production]
pip install gunicorn
pip install supervisor
```

2. **Production Configuration File**
```python
# config/production.py
import os

class ProductionConfig:
    # CrewAI Configuration
    CREWAI_VERBOSE = False
    CREWAI_MEMORY_ENABLED = True
    CREWAI_PLANNING_ENABLED = True
    CREWAI_MAX_ITERATIONS = 15
    CREWAI_MAX_EXECUTION_TIME = 600
    
    # LLM Configuration
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
    GROQ_API_KEY = os.getenv('GROQ_API_KEY')
    
    # Database Configuration
    SUPABASE_URL = os.getenv('SUPABASE_URL')
    SUPABASE_SERVICE_ROLE_KEY = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
    
    # Email Configuration
    RESEND_API_KEY = os.getenv('RESEND_API_KEY')
    
    # Security
    SECRET_KEY = os.getenv('SECRET_KEY')
    ALLOWED_HOSTS = ['yourdomain.com', 'www.yourdomain.com']
    
    # Performance
    WORKERS = 4
    TIMEOUT = 120
    MAX_REQUESTS = 1000
    MAX_REQUESTS_JITTER = 100
```

#### Frontend Deployment

1. **Build Production Bundle**
```bash
cd frontend
npm run build
npm run export
```

2. **Nginx Configuration**
```nginx
server {
    listen 80;
    server_name yourdomain.com;
    
    root /opt/aisb/frontend/out;
    index index.html;
    
    location / {
        try_files $uri $uri/ /index.html;
    }
    
    location /api {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
    
    location /static {
        alias /opt/aisb/frontend/out/static;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
```

#### CrewAI Service Management

1. **Systemd Service Configuration**
```ini
# /etc/systemd/system/aisb-crewai.service
[Unit]
Description=AISB CrewAI Service
After=network.target

[Service]
Type=exec
User=aisb
Group=aisb
WorkingDirectory=/opt/aisb
Environment=PATH=/opt/aisb/crewai-env/bin
ExecStart=/opt/aisb/crewai-env/bin/gunicorn --config gunicorn.conf.py app:app
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

2. **Gunicorn Configuration**
```python
# gunicorn.conf.py
bind = "127.0.0.1:8000"
workers = 4
worker_class = "uvicorn.workers.UvicornWorker"
worker_connections = 1000
max_requests = 1000
max_requests_jitter = 100
timeout = 120
keepalive = 5
preload_app = True
```

#### Monitoring and Logging

1. **Log Configuration**
```python
# logging.conf
[loggers]
keys=root,aisb

[handlers]
keys=consoleHandler,fileHandler

[formatters]
keys=simpleFormatter

[logger_root]
level=INFO
handlers=consoleHandler

[logger_aisb]
level=DEBUG
handlers=fileHandler
qualname=aisb
propagate=0

[handler_consoleHandler]
class=StreamHandler
level=INFO
formatter=simpleFormatter
args=(sys.stdout,)

[handler_fileHandler]
class=FileHandler
level=DEBUG
formatter=simpleFormatter
args=('/opt/aisb/logs/aisb.log',)

[formatter_simpleFormatter]
format=%(asctime)s - %(name)s - %(levelname)s - %(message)s
```

2. **Health Check Endpoint**
```python
# health_check.py
from fastapi import FastAPI
import psutil
import os

app = FastAPI()

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "system": {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_percent": psutil.disk_usage('/').percent
        },
        "crewai": {
            "agents_active": len(active_agents),
            "tasks_queued": task_queue.size(),
            "memory_usage": crewai_memory.usage()
        }
    }
```

## üîí Security Implementation

### Comprehensive Security Measures

#### Authentication & Authorization
- **JWT Token Management**: Secure token generation and validation
- **Role-Based Access Control**: Granular permission system
- **Session Management**: Secure session handling and timeout
- **Multi-Factor Authentication**: Optional 2FA implementation
- **API Key Management**: Secure API key storage and rotation

#### Data Protection
- **Encryption at Rest**: Database encryption for sensitive data
- **Encryption in Transit**: TLS/SSL for all communications
- **Data Anonymization**: PII protection and anonymization
- **GDPR Compliance**: Data protection and privacy controls
- **Audit Logging**: Comprehensive activity logging

#### Input Validation
- **Schema Validation**: Zod-based input validation
- **SQL Injection Prevention**: Parameterized queries
- **XSS Protection**: Content Security Policy implementation
- **CSRF Protection**: Cross-site request forgery prevention
- **Rate Limiting**: API endpoint protection

## üìä Performance Optimization

### CrewAI Performance Tuning

#### Agent Optimization
- **Concurrent Execution**: Parallel agent processing
- **Memory Management**: Efficient memory usage and cleanup
- **Caching Strategy**: Intelligent caching for repeated operations
- **Resource Pooling**: Shared resource management
- **Load Balancing**: Distributed processing across multiple instances

#### Database Optimization
- **Query Optimization**: Indexed queries and query analysis
- **Connection Pooling**: Efficient database connection management
- **Read Replicas**: Distributed read operations
- **Caching Layer**: Redis-based caching system
- **Batch Operations**: Bulk data processing

#### Frontend Optimization
- **Code Splitting**: Lazy loading and bundle optimization
- **Image Optimization**: Compressed and responsive images
- **CDN Integration**: Content delivery network implementation
- **Service Workers**: Offline functionality and caching
- **Performance Monitoring**: Real-time performance tracking

## üß™ Testing Strategy

### Comprehensive Testing Framework

#### Unit Testing
```python
# tests/test_quiz_generator_agent.py
import pytest
from crewai.agents.quiz_generator_agent import QuizGeneratorAgent

class TestQuizGeneratorAgent:
    def setup_method(self):
        self.agent = QuizGeneratorAgent()
    
    def test_generate_quiz_basic(self):
        """Test basic quiz generation functionality"""
        parameters = {
            'num_questions': 5,
            'difficulty': 'Medium',
            'topics': ['AI', 'Machine Learning'],
            'time_per_question': 60,
            'type': 'multiple-choice'
        }
        
        result = self.agent.generate_quiz(parameters)
        
        assert result is not None
        assert len(result['questions']) == 5
        assert all('question_text' in q for q in result['questions'])
        assert all('options' in q for q in result['questions'])
        assert all('correct_answer' in q for q in result['questions'])
    
    def test_generate_quiz_difficulty_scaling(self):
        """Test difficulty scaling functionality"""
        easy_params = {**self.base_params, 'difficulty': 'Easy'}
        hard_params = {**self.base_params, 'difficulty': 'Hard'}
        
        easy_result = self.agent.generate_quiz(easy_params)
        hard_result = self.agent.generate_quiz(hard_params)
        
        # Verify difficulty differences
        assert easy_result['average_difficulty'] < hard_result['average_difficulty']
    
    def test_error_handling(self):
        """Test error handling and recovery"""
        invalid_params = {
            'num_questions': -1,  # Invalid parameter
            'difficulty': 'Invalid',
            'topics': [],
            'time_per_question': 0,
            'type': 'invalid'
        }
        
        with pytest.raises(ValueError):
            self.agent.generate_quiz(invalid_params)
```

#### Integration Testing
```python
# tests/test_crewai_integration.py
import pytest
from crewai.crews.assessment_crew import AssessmentCrew

class TestCrewAIIntegration:
    def test_full_assessment_workflow(self):
        """Test complete assessment workflow"""
        crew = AssessmentCrew()
        
        # Test data
        student_data = {
            'name': 'Test Student',
            'email': 'test@example.com',
            'topics': ['AI', 'Machine Learning']
        }
        
        quiz_params = {
            'num_questions': 3,
            'difficulty': 'Medium',
            'topics': student_data['topics'],
            'time_per_question': 60,
            'type': 'multiple-choice'
        }
        
        # Execute full workflow
        result = crew.execute_assessment(student_data, quiz_params)
        
        # Verify results
        assert result['quiz_generated'] is True
        assert result['video_processed'] is True
        assert result['scoring_completed'] is True
        assert result['ranking_determined'] is True
        assert result['notifications_sent'] is True
```

#### End-to-End Testing
```python
# tests/test_e2e_assessment.py
import pytest
from playwright.sync_api import sync_playwright

class TestEndToEndAssessment:
    def test_complete_student_journey(self):
        """Test complete student assessment journey"""
        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            
            # Navigate to student interface
            page.goto('http://localhost:3000/student/quiz/1')
            
            # Complete quiz
            page.click('[data-testid="start-quiz"]')
            
            # Answer questions
            for i in range(3):
                page.click(f'[data-testid="option-{i}-0"]')
                page.click('[data-testid="next-question"]')
            
            # Submit quiz
            page.click('[data-testid="submit-quiz"]')
            
            # Verify submission
            assert page.locator('[data-testid="quiz-completed"]').is_visible()
            
            browser.close()
```

## üîÑ Development Workflow

### CrewAI Development Best Practices

#### Agent Development Cycle
1. **Agent Design**: Define agent roles, goals, and backstories
2. **Tool Creation**: Develop specialized tools for agent capabilities
3. **Task Definition**: Create detailed task descriptions and expected outputs
4. **Crew Assembly**: Configure agent interactions and workflow
5. **Testing**: Comprehensive unit and integration testing
6. **Deployment**: Production deployment with monitoring

#### Code Quality Standards
- **Type Safety**: Full TypeScript implementation with strict checking
- **Error Handling**: Comprehensive error detection and recovery
- **Logging**: Structured logging for debugging and monitoring
- **Documentation**: Inline documentation and API documentation
- **Testing**: Minimum 80% code coverage requirement

#### Version Control Strategy
```bash
# Feature branch workflow
git checkout -b feature/crewai-agent-enhancement
git add .
git commit -m "feat: enhance quiz generator agent with advanced capabilities"
git push origin feature/crewai-agent-enhancement

# Create pull request with detailed description
# Include:
# - Agent configuration changes
# - Tool modifications
# - Testing results
# - Performance impact analysis
```

## üìà Monitoring & Analytics

### CrewAI Performance Monitoring

#### Agent Performance Metrics
- **Execution Time**: Average and maximum execution times per agent
- **Success Rate**: Percentage of successful task completions
- **Error Rate**: Frequency and types of errors encountered
- **Resource Usage**: CPU, memory, and API call consumption
- **Throughput**: Tasks processed per minute/hour

#### Real-time Dashboard
```python
# monitoring/dashboard.py
from crewai import Crew
import streamlit as st
import plotly.express as px
import pandas as pd

class CrewAIMonitoringDashboard:
    def __init__(self):
        self.crew = Crew()
        self.metrics_collector = MetricsCollector()
    
    def display_agent_performance(self):
        """Display real-time agent performance metrics"""
        metrics = self.metrics_collector.get_agent_metrics()
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Active Agents", metrics['active_agents'])
        
        with col2:
            st.metric("Tasks Completed", metrics['tasks_completed'])
        
        with col3:
            st.metric("Success Rate", f"{metrics['success_rate']:.1f}%")
        
        with col4:
            st.metric("Avg Execution Time", f"{metrics['avg_execution_time']:.2f}s")
    
    def display_task_queue(self):
        """Display current task queue status"""
        queue_status = self.metrics_collector.get_queue_status()
        
        st.subheader("Task Queue Status")
        
        # Create task queue visualization
        fig = px.bar(
            queue_status,
            x='agent_type',
            y='queue_size',
            title='Tasks in Queue by Agent Type'
        )
        st.plotly_chart(fig)
    
    def display_error_analysis(self):
        """Display error analysis and trends"""
        error_data = self.metrics_collector.get_error_analysis()
        
        st.subheader("Error Analysis")
        
        # Error trend chart
        fig = px.line(
            error_data,
            x='timestamp',
            y='error_count',
            title='Error Trends Over Time'
        )
        st.plotly_chart(fig)
```

#### Alert System
```python
# monitoring/alerts.py
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class CrewAIAlertSystem:
    def __init__(self):
        self.alert_thresholds = {
            'error_rate': 0.05,  # 5% error rate threshold
            'execution_time': 300,  # 5 minutes execution time threshold
            'queue_size': 100,  # 100 tasks in queue threshold
            'memory_usage': 0.8  # 80% memory usage threshold
        }
    
    def check_thresholds(self, metrics):
        """Check if any metrics exceed alert thresholds"""
        alerts = []
        
        if metrics['error_rate'] > self.alert_thresholds['error_rate']:
            alerts.append({
                'type': 'error_rate',
                'message': f"Error rate {metrics['error_rate']:.2%} exceeds threshold",
                'severity': 'high'
            })
        
        if metrics['avg_execution_time'] > self.alert_thresholds['execution_time']:
            alerts.append({
                'type': 'execution_time',
                'message': f"Average execution time {metrics['avg_execution_time']:.2f}s exceeds threshold",
                'severity': 'medium'
            })
        
        if metrics['queue_size'] > self.alert_thresholds['queue_size']:
            alerts.append({
                'type': 'queue_size',
                'message': f"Queue size {metrics['queue_size']} exceeds threshold",
                'severity': 'medium'
            })
        
        return alerts
    
    def send_alert(self, alert):
        """Send alert notification"""
        if alert['severity'] == 'high':
            self._send_email_alert(alert)
            self._send_slack_alert(alert)
        elif alert['severity'] == 'medium':
            self._send_slack_alert(alert)
    
    def _send_email_alert(self, alert):
        """Send email alert to administrators"""
        msg = MIMEMultipart()
        msg['From'] = "alerts@aisb-platform.com"
        msg['To'] = "admin@aisb-platform.com"
        msg['Subject'] = f"CrewAI Alert: {alert['type']}"
        
        body = f"""
        CrewAI System Alert
        
        Type: {alert['type']}
        Message: {alert['message']}
        Severity: {alert['severity']}
        Timestamp: {datetime.now().isoformat()}
        
        Please investigate this issue immediately.
        """
        
        msg.attach(MIMEText(body, 'plain'))
        
        # Send email (implementation details)
        pass
    
    def _send_slack_alert(self, alert):
        """Send Slack alert to team channel"""
        # Slack webhook implementation
        pass
```

## üîß Troubleshooting Guide

### Common CrewAI Issues and Solutions

#### Agent Execution Issues

**Problem**: Agent fails to execute tasks
```python
# Debug agent execution
def debug_agent_execution(agent, task):
    try:
        # Enable verbose logging
        agent.verbose = True
        
        # Check agent configuration
        print(f"Agent Role: {agent.role}")
        print(f"Agent Goal: {agent.goal}")
        print(f"Available Tools: {[tool.name for tool in agent.tools]}")
        
        # Execute with detailed logging
        result = agent.execute(task)
        return result
        
    except Exception as e:
        print(f"Agent execution error: {str(e)}")
        
        # Check common issues
        if "API key" in str(e):
            print("Solution: Verify API key configuration")
        elif "rate limit" in str(e):
            print("Solution: Implement rate limiting and retry logic")
        elif "memory" in str(e):
            print("Solution: Increase memory allocation or optimize agent memory usage")
        
        raise e
```

**Problem**: Tool execution failures
```python
# Debug tool execution
def debug_tool_execution(tool, input_data):
    try:
        # Validate input schema
        validated_input = tool.args_schema(**input_data)
        
        # Execute tool with error handling
        result = tool._run(**validated_input.dict())
        return result
        
    except ValidationError as e:
        print(f"Input validation error: {str(e)}")
        print("Solution: Check input data format and required fields")
        
    except Exception as e:
        print(f"Tool execution error: {str(e)}")
        
        # Check tool-specific issues
        if "API" in str(e):
            print("Solution: Verify external API connectivity and credentials")
        elif "timeout" in str(e):
            print("Solution: Increase timeout settings or optimize tool performance")
        
        raise e
```

#### Performance Optimization

**Problem**: Slow agent execution
```python
# Performance optimization strategies
class PerformanceOptimizer:
    def __init__(self):
        self.cache = {}
        self.performance_metrics = {}
    
    def optimize_agent_performance(self, agent):
        """Optimize agent performance"""
        optimizations = []
        
        # 1. Enable caching for repeated operations
        if hasattr(agent, 'enable_caching'):
            agent.enable_caching = True
            optimizations.append("Enabled agent caching")
        
        # 2. Optimize tool selection
        if len(agent.tools) > 5:
            # Prioritize most frequently used tools
            prioritized_tools = self._prioritize_tools(agent.tools)
            agent.tools = prioritized_tools[:5]
            optimizations.append("Optimized tool selection")
        
        # 3. Adjust LLM parameters
        if hasattr(agent, 'llm'):
            agent.llm.temperature = 0.3  # Reduce randomness for faster execution
            agent.llm.max_tokens = 1000  # Limit response length
            optimizations.append("Optimized LLM parameters")
        
        # 4. Enable parallel execution
        if hasattr(agent, 'allow_delegation'):
            agent.allow_delegation = True
            optimizations.append("Enabled parallel execution")
        
        return optimizations
    
    def _prioritize_tools(self, tools):
        """Prioritize tools based on usage frequency"""
        # Implementation would analyze tool usage patterns
        return sorted(tools, key=lambda t: self._get_tool_usage_frequency(t.name), reverse=True)
    
    def _get_tool_usage_frequency(self, tool_name):
        """Get tool usage frequency from metrics"""
        return self.performance_metrics.get(tool_name, 0)
```

#### Memory Management

**Problem**: High memory usage
```python
# Memory management strategies
class MemoryManager:
    def __init__(self):
        self.memory_threshold = 0.8  # 80% memory usage threshold
        self.cleanup_interval = 300  # 5 minutes cleanup interval
    
    def monitor_memory_usage(self):
        """Monitor and manage memory usage"""
        import psutil
        
        memory_percent = psutil.virtual_memory().percent
        
        if memory_percent > self.memory_threshold * 100:
            self._cleanup_memory()
            return True
        
        return False
    
    def _cleanup_memory(self):
        """Clean up memory resources"""
        # 1. Clear agent conversation history
        self._clear_agent_history()
        
        # 2. Clear tool caches
        self._clear_tool_caches()
        
        # 3. Garbage collection
        import gc
        gc.collect()
        
        print("Memory cleanup completed")
    
    def _clear_agent_history(self):
        """Clear agent conversation history"""
        # Implementation would clear agent memory
        pass
    
    def _clear_tool_caches(self):
        """Clear tool caches"""
        # Implementation would clear tool caches
        pass
```

## üöÄ Advanced CrewAI Features

### Custom Agent Development

#### Creating Specialized Agents
```python
# agents/specialized_quiz_agent.py
from crewai import Agent
from langchain_openai import ChatOpenAI
from typing import Dict, List, Any
import json

class SpecializedQuizAgent(Agent):
    def __init__(self, specialization: str = "general"):
        self.specialization = specialization
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0.7,
            max_tokens=2000
        )
        
        super().__init__(
            role=f'{specialization.title()} Quiz Specialist',
            goal=f'Generate high-quality {specialization} quizzes with expert-level content',
            backstory=f"""You are a world-renowned expert in {specialization} with over 20 years 
            of experience in education and assessment. You have published numerous papers on 
            {specialization} pedagogy and have designed assessments for top universities worldwide.""",
            verbose=True,
            allow_delegation=False,
            tools=[self._create_specialized_tools()],
            llm=self.llm
        )
    
    def _create_specialized_tools(self):
        """Create specialized tools based on agent specialization"""
        if self.specialization == "mathematics":
            return [MathQuizTool(), FormulaValidationTool()]
        elif self.specialization == "science":
            return [ScienceQuizTool(), ExperimentAnalysisTool()]
        elif self.specialization == "programming":
            return [CodeQuizTool(), CodeExecutionTool()]
        else:
            return [GeneralQuizTool()]
    
    def generate_specialized_quiz(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Generate quiz with specialization-specific features"""
        # Add specialization-specific parameters
        enhanced_params = self._enhance_parameters(parameters)
        
        # Generate quiz using specialized knowledge
        quiz_data = self._generate_with_specialization(enhanced_params)
        
        # Validate quiz quality
        validated_quiz = self._validate_quiz_quality(quiz_data)
        
        return validated_quiz
    
    def _enhance_parameters(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Enhance parameters with specialization-specific settings"""
        enhanced = parameters.copy()
        
        if self.specialization == "mathematics":
            enhanced['include_formulas'] = True
            enhanced['require_work_shown'] = True
            enhanced['difficulty_progression'] = 'linear'
        
        elif self.specialization == "science":
            enhanced['include_diagrams'] = True
            enhanced['require_explanations'] = True
            enhanced['lab_scenarios'] = True
        
        elif self.specialization == "programming":
            enhanced['include_code_samples'] = True
            enhanced['require_debugging'] = True
            enhanced['multiple_languages'] = True
        
        return enhanced
    
    def _generate_with_specialization(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Generate quiz using specialization knowledge"""
        prompt = self._create_specialized_prompt(parameters)
        
        response = self.llm.invoke(prompt)
        
        return self._parse_specialized_response(response.content)
    
    def _create_specialized_prompt(self, parameters: Dict[str, Any]) -> str:
        """Create specialization-specific prompt"""
        base_prompt = f"""
        Generate a {parameters['num_questions']}-question quiz about {', '.join(parameters['topics'])}.
        Difficulty: {parameters['difficulty']}
        Specialization: {self.specialization}
        """
        
        if self.specialization == "mathematics":
            base_prompt += """
            Requirements:
            - Include step-by-step solutions
            - Use proper mathematical notation
            - Test both computational and conceptual understanding
            - Include word problems that require problem-solving
            """
        
        elif self.specialization == "science":
            base_prompt += """
            Requirements:
            - Include scientific diagrams where appropriate
            - Test understanding of scientific principles
            - Include experimental scenarios
            - Test both theoretical and practical knowledge
            """
        
        elif self.specialization == "programming":
            base_prompt += """
            Requirements:
            - Include code snippets for analysis
            - Test debugging skills
            - Include algorithm design questions
            - Test understanding of multiple programming concepts
            """
        
        return base_prompt
    
    def _parse_specialized_response(self, response: str) -> Dict[str, Any]:
        """Parse response with specialization-specific handling"""
        try:
            quiz_data = json.loads(response)
            
            # Add specialization-specific metadata
            quiz_data['specialization'] = self.specialization
            quiz_data['generated_at'] = datetime.now().isoformat()
            quiz_data['agent_version'] = '1.0.0'
            
            return quiz_data
            
        except json.JSONDecodeError:
            # Handle parsing errors with specialization-specific recovery
            return self._recover_from_parsing_error(response)
    
    def _recover_from_parsing_error(self, response: str) -> Dict[str, Any]:
        """Recover from parsing errors"""
        # Implementation would attempt to extract valid JSON
        # and provide fallback quiz generation
        pass
    
    def _validate_quiz_quality(self, quiz_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate quiz quality with specialization-specific criteria"""
        validation_results = {
            'is_valid': True,
            'quality_score': 0,
            'issues': [],
            'recommendations': []
        }
        
        # Specialization-specific validation
        if self.specialization == "mathematics":
            validation_results.update(self._validate_math_quiz(quiz_data))
        elif self.specialization == "science":
            validation_results.update(self._validate_science_quiz(quiz_data))
        elif self.specialization == "programming":
            validation_results.update(self._validate_programming_quiz(quiz_data))
        
        return quiz_data
    
    def _validate_math_quiz(self, quiz_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate mathematics quiz specific criteria"""
        # Implementation would validate math-specific criteria
        return {}
    
    def _validate_science_quiz(self, quiz_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate science quiz specific criteria"""
        # Implementation would validate science-specific criteria
        return {}
    
    def _validate_programming_quiz(self, quiz_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate programming quiz specific criteria"""
        # Implementation would validate programming-specific criteria
        return {}
```

## üìö Additional Resources

### CrewAI Documentation Links
- [CrewAI Official Documentation](https://docs.crewai.com/)
- [CrewAI GitHub Repository](https://github.com/joaomdmoura/crewAI)
- [CrewAI Examples](https://github.com/joaomdmoura/crewAI/tree/main/examples)
- [CrewAI Community](https://discord.gg/crewai)

### Learning Resources
- **CrewAI Tutorial Series**: Comprehensive video tutorials on CrewAI implementation
- **Agent Design Patterns**: Best practices for designing effective AI agents
- **Tool Development Guide**: Advanced tool creation and optimization techniques
- **Performance Optimization**: Strategies for improving agent and crew performance

### Community Support
- **Discord Server**: Real-time community support and discussions
- **GitHub Issues**: Bug reports and feature requests
- **Stack Overflow**: Technical questions and answers
- **Reddit Community**: General discussions and project showcases

### Professional Services
- **CrewAI Consulting**: Professional implementation and optimization services
- **Custom Agent Development**: Specialized agent creation for specific use cases
- **Training Workshops**: Hands-on training sessions for teams
- **Enterprise Support**: Dedicated support for enterprise implementations

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ü§ù Contributing

We welcome contributions to the AI Skill Bridge platform! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details on how to:

1. **Fork the repository**
2. **Create a feature branch**
3. **Implement changes with proper testing**
4. **Submit a pull request with detailed description**

### Contribution Areas
- **Agent Development**: New specialized agents for different domains
- **Tool Enhancement**: Advanced tools with additional capabilities
- **Performance Optimization**: Improvements to agent and crew performance
- **Documentation**: Enhanced documentation and examples
- **Testing**: Additional test coverage and quality assurance

## üìû Support & Contact

### Technical Support
- **Email**: support@aisb-platform.com
- **Documentation**: [docs.aisb-platform.com](https://docs.aisb-platform.com)
- **Issue Tracker**: [GitHub Issues](https://github.com/your-org/aisb-auto/issues)

### Business Inquiries
- **Email**: business@aisb-platform.com
- **Phone**: +1 (555) 123-4567
- **LinkedIn**: [AI Skill Bridge](https://linkedin.com/company/aisb-platform)

### Community
- **Discord**: [Join our community](https://discord.gg/aisb-platform)
- **Twitter**: [@AISBPlatform](https://twitter.com/AISBPlatform)
- **YouTube**: [AI Skill Bridge Channel](https://youtube.com/@aisb-platform)

---

**Built with ‚ù§Ô∏è using CrewAI, Next.js, and cutting-edge AI technologies**

*Empowering education through intelligent multi-agent systems*
